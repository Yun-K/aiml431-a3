{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge (5 point)\n",
    "\n",
    "Improving classification spatial awareness by taking into account neighbouring pixels. This will involve:\n",
    "\n",
    "- Creation and training of a 2D CNN classifier within a Jupyter notebook which takes into account the neighbouring pixels of HSI data.\n",
    "- Test at least two 2D convolution filters sizes (e.g. 3x3, 5x5) and outline their corresponding performances.\n",
    "- Tune the hyperparameters in order to achieve the optimal trade-off between model performance and training time.\n",
    "- Present the output from you 2D CNN and compare with cross-correlation, DT, 1D CNN or MLP.\n",
    "- Discuss potential future avenues for improving classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can use: cuda to run the Model \n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "import torchmetrics  # metrics like accuracy, recall, etc\n",
    "import torch.nn as nn\n",
    "# from torch import nn\n",
    "from torch.nn.modules.container import Sequential\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn import decomposition, datasets\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from copy import deepcopy\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"we can use:\", DEVICE, \"to run the Model \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results.shape (202, 202, 11)\n",
      "Hyperspectral data: x_total: 202 y_total: 202 wavelengh: 79\n",
      "Resized Hyperspectra data shape : (202, 202, 21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dataset_path = sys.argv[1];\n",
    "#folder = sys.argv[2];\n",
    "#filename = sys.argv[3];\n",
    "\n",
    "dataset_path = \"MC_Modeled_spectra.csv\"; # monte carlo dataset \n",
    "folder = \"Data_Mike\" + \"/\";\n",
    "filename = \"R_(1measurement)_normalized_NoGap[202,202,79].mat\"; # hyperspectral image(pixels, wavelengths)\n",
    "\n",
    "\n",
    "HSI_mat_data_file_to_load = folder + filename; # HSI file\n",
    "# copy from the given code, it will be saved by using np.save() with the format of .npy\n",
    "saved_Results_file = folder + filename + dataset_path; \n",
    "provided_npy_file = \"R_(1measurement)_normalized_NoGap[202,202,79].matData_small.csv.npy\"\n",
    "Results_from_given_npy = np.load(provided_npy_file, allow_pickle=True)\n",
    "print(\"Results.shape\", Results_from_given_npy.shape)\n",
    "\n",
    "Hyperspectra_data = loadmat(HSI_mat_data_file_to_load)['R1'];\n",
    "(x_total, y_total, wavelenght) = Hyperspectra_data.shape;\n",
    "print(\"Hyperspectral data: x_total:\", x_total, \"y_total:\", y_total, \"wavelengh:\", wavelenght);\n",
    "\n",
    "Hyperspectra_data_resized = Hyperspectra_data[:,:, 0:42:2]; # Get from 510 to 720 nm with 10nm step size\n",
    "print(f\"Resized Hyperspectra data shape : {Hyperspectra_data_resized.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    squared_error = (y_pred - y_true) ** 2\n",
    "    sum_squared_error = np.sum(squared_error)\n",
    "    mse_loss = sum_squared_error / y_true.size\n",
    "    sse = sum((y_true - y_pred)**2)\n",
    "    tse = (len(y_true) - 1) * np.var(y_true, ddof=1)\n",
    "    r2_score = 1 - (sse / tse)    \n",
    "    return mse_loss, r2_score, sse, tse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the 2D CNN model\n",
    "\n",
    "- Creation and training of a 2D CNN classifier within a Jupyter notebook which takes into account the neighbouring pixels of HSI data.\n",
    "- Test at least two 2D convolution filters sizes (e.g. 3x3, 5x5) and outline their corresponding performances.\n",
    "- Tune the hyperparameters in order to achieve the optimal trade-off between model performance and training time.\n",
    "- Present the output from you 2D CNN and compare with cross-correlation, DT, 1D CNN or MLP.\n",
    "- Discuss potential future avenues for improving classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.kaggle.com/artgor/simple-eda-and-model-in-pytorch/notebook\n",
    "# # https://www.mashen.zone/thread-1825047.htm\n",
    "# # https://blog.csdn.net/nanke_4869/article/details/113458729\n",
    "# # CNN model\n",
    "# something has to be commented out since no enough GPU memory in Colab\n",
    "class CNN(nn.Module):\n",
    "  \"\"\"\n",
    "https://datascience.stackexchange.com/questions/40906/determining-size-of-fc-layer-after-conv-layer-in-pytorch\n",
    "  https://towardsdatascience.com/classification-of-fruit-images-using-neural-networks-pytorch-1d34d49342c7\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      self.network = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "          nn.Dropout(0.25),\n",
    "          nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(inplace=True),\n",
    "          \n",
    "          \n",
    "        #   nn.Linear(32 * 101 * 101, 128),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Dropout(0.5),\n",
    "        #   nn.Linear(128, 2)\n",
    "          \n",
    "      )\n",
    "      \n",
    "  def forward(self, xb):\n",
    "      return self.network(xb)\n",
    "\n",
    "\n",
    "  def to_model_string(self):\n",
    "      return 'CNN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (218700, 25), y.shape: (218700,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/pkg/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 32, 2, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda12/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m mel_optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(mel_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda12/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m mel_loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcuda12/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m mel_val_loss, mel_model \u001b[39m=\u001b[39m train_model(mel_model, mel_trainLoader, mel_testLoader, mel_optimizer, mel_loss_fn, \u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda12/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda12/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcuda12/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda12/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda12/home/zhouyun/Desktop/aiml431-a3/A3_Framework/CNN_2D.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/torch/nn/modules/loss.py:520\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 520\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/torch/nn/functional.py:3111\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3108\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3109\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3111\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3112\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/usr/pkg/lib/python3.9/site-packages/torch/functional.py:72\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "def get_data_loader(ds, y_name, batch_size):\n",
    "    X = ds.iloc[:, 46:67].values\n",
    "    y = ds[y_name].values\n",
    "    # add 3 zerovalued columns to X , to make it 25 columns\n",
    "    # so 25 columns can be reshaped to 5x5 matrix and can be used as input to CNN\n",
    "    X = np.hstack((X, np.zeros((X.shape[0], 4))))\n",
    "    # X = X.reshape(-1, 1, 5, 5)\n",
    "    print(f\"X.shape: {X.shape}, y.shape: {y.shape}\")\n",
    "    X = torch.from_numpy(X).float()\n",
    "    y = torch.from_numpy(y).float()\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    \n",
    "    train_set, valid_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    # Generate predictions\n",
    "    pred = model(xb)\n",
    "    # Calculate loss\n",
    "    loss = loss_func(pred, yb)\n",
    "    if opt is not None:\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Update parameters\n",
    "        opt.step()\n",
    "        # Reset gradients\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "            X, y = batch\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            # print(f\"X.shape: {X.shape}, y.shape: {y.shape}\")\n",
    "            # need to handle RuntimeError that expecteding 4D input, but got 2D input\n",
    "            X = X.reshape(-1, 1, 5, 5)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_fn, X, y, opt=None) for X, y in val_loader]\n",
    "            )\n",
    "            val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print(f\"Epoch: {epoch}, val_loss: {val_loss}\")\n",
    "    return val_loss , model\n",
    "\n",
    "ds = pd.read_csv(dataset_path)\n",
    "mel_trainLoader, mel_testLoader = get_data_loader(ds, 'Mel', 32)\n",
    "mel_model = CNN().to(DEVICE)\n",
    "mel_optimizer = torch.optim.Adam(mel_model.parameters(), lr=0.001)\n",
    "mel_loss_fn = nn.MSELoss()\n",
    "mel_val_loss, mel_model = train_model(mel_model, mel_trainLoader, mel_testLoader, mel_optimizer, mel_loss_fn, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(dataset_path)\n",
    "blsdbn_trainLoader, blsdbn_testLoader = get_data_loader(ds, 'BlSDBN', 64)\n",
    "blsdbn_model = CNN().to(DEVICE)\n",
    "blsdbn_optimizer = torch.optim.Adam(blsdbn_model.parameters(), lr=0.001)\n",
    "blsdbn_loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_preds = []\n",
    "        for batch in test_loader:\n",
    "            X, y = batch\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            y_pred = model(X)\n",
    "            y_preds.append(y_pred)\n",
    "    return y_preds\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52da647447b8fe2208076266408c42f82750713fb5b92055dee0a0742687bf52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
